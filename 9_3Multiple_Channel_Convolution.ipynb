{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaeohshin/pytorch/blob/main/9_3Multiple_Channel_Convolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf_4Oda7YBV7"
      },
      "source": [
        "<p style=\"text-align:center\">\n",
        "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01\" target=\"_blank\">\n",
        "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
        "    </a>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "<h1>Multiple Input and Output Channels</h1> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ_ShJNXYBV9"
      },
      "source": [
        "\n",
        "<h3>Objective for this Notebook<h3>    \n",
        "<h5> 1. Learn on Multiple Input and Multiple Output Channels.</h5>    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxPg-rpGYBV9"
      },
      "source": [
        "\n",
        "# Table of Contents\n",
        "In this lab, you will study convolution and review how the different operations change the relationship between input and output.\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "<li><a href=\"#ref0\">Multiple Output Channels </a></li>\n",
        "\n",
        "<li><a href=\"#ref1\">Multiple Inputs</a></li>\n",
        "<li><a href=\"#ref2\">Multiple Input and Multiple Output Channels </a></li>\n",
        "<li><a href=\"#ref3\">Practice Questions </a></li>\n",
        "\n",
        "<br>\n",
        "<p></p>\n",
        "Estimated Time Needed: <strong>25 min</strong>\n",
        "</div>\n",
        "\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBd940WkYBV-"
      },
      "source": [
        "Import the following libraries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoSwBExAYBV-"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy import ndimage, misc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA1Mjr7wYBV_"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYdVyhv8YBV_"
      },
      "source": [
        "<a id=\"ref0\"></a>\n",
        "<h2 align=center>Multiple Output Channels </h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvQx2ULoYBV_"
      },
      "source": [
        "In Pytroch, you can create a <code>Conv2d</code> object with multiple outputs. For each channel, a kernel is created, and each kernel performs a convolution independently. As a result, the number of outputs is equal to the number of channels. This is demonstrated in the following figure. The number 9 is convolved with three kernels: each of a different color. There are three different activation maps represented by the different colors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4IjmsjqYBWA"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2activationmaps.png\" width=\"750,\" align=\"center\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k5yen9lYBWA"
      },
      "source": [
        "Symbolically, this can be represented as follows:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGcgSp-MYBWA"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2activationmap2.png\" width=\"500,\" align=\"center\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvmojO1zYBWA"
      },
      "source": [
        "Create a <code>Conv2d</code> with three channels:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbmFP7-bYBWA"
      },
      "outputs": [],
      "source": [
        "conv1 = nn.Conv2d(in_channels=1, out_channels=3,kernel_size=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoBF2-XVYBWB"
      },
      "source": [
        "Pytorch randomly assigns values to each kernel. However, use kernels that have  been developed to detect edges:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjRL47GlYBWB"
      },
      "outputs": [],
      "source": [
        "Gx=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])\n",
        "Gy=torch.tensor([[1.0,2.0,1.0],[0.0,0.0,0.0],[-1.0,-2.0,-1.0]])\n",
        "\n",
        "conv1.state_dict()['weight'][0][0]=Gx\n",
        "conv1.state_dict()['weight'][1][0]=Gy\n",
        "conv1.state_dict()['weight'][2][0]=torch.ones(3,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xe-hleWYBWB"
      },
      "source": [
        "Each kernel has its own bias, so set them all to zero:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA292aMMYBWB"
      },
      "outputs": [],
      "source": [
        "conv1.state_dict()['bias'][:]=torch.tensor([0.0,0.0,0.0])\n",
        "conv1.state_dict()['bias']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrgiuX78YBWB"
      },
      "source": [
        "Print out each kernel: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWV3EZ3SYBWC"
      },
      "outputs": [],
      "source": [
        "for x in conv1.state_dict()['weight']:\n",
        "    print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2x_nbm_NYBWC"
      },
      "source": [
        "Create an input <code>image</code> to represent the input X:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUtNrtdLYBWC"
      },
      "outputs": [],
      "source": [
        "image=torch.zeros(1,1,5,5)\n",
        "image[0,0,:,2]=1\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4NX95t7YBWC"
      },
      "source": [
        "Plot it as an image: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Tbea3kNYBWC"
      },
      "outputs": [],
      "source": [
        "plt.imshow(image[0,0,:,:].numpy(), interpolation='nearest', cmap=plt.cm.gray)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hreq9n84YBWC"
      },
      "source": [
        "Perform convolution using each channel: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ATQ90dQYBWC"
      },
      "outputs": [],
      "source": [
        "out=conv1(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sogaokwYBWD"
      },
      "source": [
        "The result is a 1x3x3x3 tensor. This represents one sample with three channels, and each channel contains a 3x3 image.  The same rules that govern the shape of each image were discussed in the last section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNO1ikeSYBWD"
      },
      "outputs": [],
      "source": [
        "out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpCszHJRYBWD"
      },
      "source": [
        "Print out each channel as a tensor or an image: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDJJibmTYBWD"
      },
      "outputs": [],
      "source": [
        "for channel,image in enumerate(out[0]):\n",
        "    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n",
        "    print(image)\n",
        "    plt.title(\"channel {}\".format(channel))\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ugs9vKLYBWE"
      },
      "source": [
        "Different kernels can be used to detect various features in an image. You can see that the first channel fluctuates, and the second two channels produce a constant value. The following figure summarizes the process:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C4lKqfiYBWE"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2outputsgray.png\" width=\"750,\" align=\"center\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq2y2w6bYBWE"
      },
      "source": [
        "If you use a different image, the result will be different: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4iwIqE-YBWE"
      },
      "outputs": [],
      "source": [
        "image1=torch.zeros(1,1,5,5)\n",
        "image1[0,0,2,:]=1\n",
        "print(image1)\n",
        "plt.imshow(image1[0,0,:,:].detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oFYec9DYBWE"
      },
      "source": [
        "In this case, the second channel fluctuates, and the first and the third channels produce a constant value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PXXVGFhYBWE"
      },
      "outputs": [],
      "source": [
        "out1=conv1(image1)\n",
        "for channel,image in enumerate(out1[0]):\n",
        "    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n",
        "    print(image)\n",
        "    plt.title(\"channel {}\".format(channel))\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMz2PZxXYBWF"
      },
      "source": [
        "The following figure summarizes the process:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbAozRSKYBWF"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2ouputsgray2.png\" width=\"750,\" align=\"center\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "golCKncTYBWF"
      },
      "source": [
        "<a id=\"ref1\"></a>\n",
        "<h2 align=center>Multiple Input Channels </h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02uvle3dYBWF"
      },
      "source": [
        "For two inputs, you can create two kernels. Each kernel performs a convolution on its associated input channel. The resulting output is added together as shown:  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6kBD-mbYBWF"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.22chanalsinput.png\" width=\"750,\" align=\"center\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksvqF9SIYBWF"
      },
      "source": [
        "Create an input with two channels:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23PBrp_6YBWF"
      },
      "outputs": [],
      "source": [
        "image2=torch.zeros(1,2,5,5)\n",
        "image2[0,0,2,:]=-2\n",
        "image2[0,1,2,:]=1\n",
        "image2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfvnbjPeYBWG"
      },
      "source": [
        "Plot out each image: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJO7702zYBWG"
      },
      "outputs": [],
      "source": [
        "for channel,image in enumerate(image2[0]):\n",
        "    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n",
        "    print(image)\n",
        "    plt.title(\"channel {}\".format(channel))\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clbbGvYqYBWG"
      },
      "source": [
        "Create a <code>Conv2d</code> object with two inputs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQE_k6bzYBWG"
      },
      "outputs": [],
      "source": [
        "conv3 = nn.Conv2d(in_channels=2, out_channels=1,kernel_size=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXyN40cVYBWG"
      },
      "source": [
        "Assign kernel values to make the math a little easier: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bG0iPUSpYBWG"
      },
      "outputs": [],
      "source": [
        "Gx1=torch.tensor([[0.0,0.0,0.0],[0,1.0,0],[0.0,0.0,0.0]])\n",
        "conv3.state_dict()['weight'][0][0]=1*Gx1\n",
        "conv3.state_dict()['weight'][0][1]=-2*Gx1\n",
        "conv3.state_dict()['bias'][:]=torch.tensor([0.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRopjuHIYBWH"
      },
      "outputs": [],
      "source": [
        "conv3.state_dict()['weight']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-g2YFg7YBWH"
      },
      "source": [
        "Perform the convolution:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VF7TFMLdYBWH"
      },
      "outputs": [],
      "source": [
        "conv3(image2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0yQ7VtdYBWH"
      },
      "source": [
        "The following images summarize the process. The object performs Convolution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgO6Fiv2YBWH"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_two_channal_example.png\" width=\"750,\" align=\"center\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2QMcxFqYBWH"
      },
      "source": [
        "Then, it adds the result: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2VuVHZwYBWH"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_two_channal_example2.png\" width=\"750,\" align=\"center\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECpuzlvhYBWI"
      },
      "source": [
        "<a id=\"ref2\"></a>\n",
        "\n",
        "<h2>Multiple Input and Multiple Output Channels</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suST8V6tYBWI"
      },
      "source": [
        "When using multiple inputs and outputs, a kernel is created for each input, and the process is repeated for each output. The process is summarized in the following image. \n",
        "\n",
        "There are two input channels and 3 output channels. For each channel, the input in red and purple is convolved with an individual kernel that is colored differently. As a result, there are three outputs. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdDj-PIOYBWI"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2mulit_input_output.png\" width=\"750,\" align=\"center\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSWM5sKoYBWI"
      },
      "source": [
        "Create an example with two inputs and three outputs and assign the kernel values to make the math a little easier: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "js-Hcu-3YBWI"
      },
      "outputs": [],
      "source": [
        "conv4 = nn.Conv2d(in_channels=2, out_channels=3,kernel_size=3)\n",
        "conv4.state_dict()['weight'][0][0]=torch.tensor([[0.0,0.0,0.0],[0,0.5,0],[0.0,0.0,0.0]])\n",
        "conv4.state_dict()['weight'][0][1]=torch.tensor([[0.0,0.0,0.0],[0,0.5,0],[0.0,0.0,0.0]])\n",
        "\n",
        "\n",
        "conv4.state_dict()['weight'][1][0]=torch.tensor([[0.0,0.0,0.0],[0,1,0],[0.0,0.0,0.0]])\n",
        "conv4.state_dict()['weight'][1][1]=torch.tensor([[0.0,0.0,0.0],[0,-1,0],[0.0,0.0,0.0]])\n",
        "\n",
        "conv4.state_dict()['weight'][2][0]=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])\n",
        "conv4.state_dict()['weight'][2][1]=torch.tensor([[1.0,2.0,1.0],[0.0,0.0,0.0],[-1.0,-2.0,-1.0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLaVqjPxYBWI"
      },
      "source": [
        "For each output, there is a bias, so set them all to zero: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVSffxidYBWI"
      },
      "outputs": [],
      "source": [
        "conv4.state_dict()['bias'][:]=torch.tensor([0.0,0.0,0.0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYvzP96iYBWJ"
      },
      "source": [
        "Create a two-channel image and plot the results: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOzumH66YBWJ"
      },
      "outputs": [],
      "source": [
        "image4=torch.zeros(1,2,5,5)\n",
        "image4[0][0]=torch.ones(5,5)\n",
        "image4[0][1][2][2]=1\n",
        "for channel,image in enumerate(image4[0]):\n",
        "    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n",
        "    print(image)\n",
        "    plt.title(\"channel {}\".format(channel))\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4QJaLe3YBWJ"
      },
      "source": [
        "Perform the convolution:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o55Tk2UOYBWJ"
      },
      "outputs": [],
      "source": [
        "z=conv4(image4)\n",
        "z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgAq44X6YBWJ"
      },
      "source": [
        "The output of the first channel is given by: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKiHan0nYBWJ"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_%20multi_channel_1.png\" width=\"750,\" align=\"center\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfYjVmMMYBWJ"
      },
      "source": [
        "The output of the second channel is given by:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DjpUghsYBWK"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_%20multi_channel_2.png\" width=\"750,\" align=\"center\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghuVuobEYBWK"
      },
      "source": [
        "The output of the third channel is given by: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsTQuz50YBWK"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2_%20multi_channel_3.png\" width=\"750,\" align=\"center\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH00PTOLYBWK"
      },
      "source": [
        "<a id=\"ref3\"></a>\n",
        "\n",
        "<h2>Practice Questions </h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tly-Kup3YBWK"
      },
      "source": [
        "Use the following two convolution objects to produce the same result as two input channel convolution on imageA and imageB as shown in the following image:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "An4dzsaNYBWK"
      },
      "outputs": [],
      "source": [
        "imageA=torch.zeros(1,1,5,5)\n",
        "imageB=torch.zeros(1,1,5,5)\n",
        "imageA[0,0,2,:]=-2\n",
        "imageB[0,0,2,:]=1\n",
        "\n",
        "\n",
        "conv5 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\n",
        "conv6 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\n",
        "\n",
        "\n",
        "Gx1=torch.tensor([[0.0,0.0,0.0],[0,1.0,0],[0.0,0.0,0.0]])\n",
        "conv5.state_dict()['weight'][0][0]=1*Gx1\n",
        "conv6.state_dict()['weight'][0][0]=-2*Gx1\n",
        "conv5.state_dict()['bias'][:]=torch.tensor([0.0])\n",
        "conv6.state_dict()['bias'][:]=torch.tensor([0.0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1_Lu9oAYBWK"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2Practice%20Questions_1.png\" width=\"750,\" align=\"center\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88vuiKljYBWL"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.1.2Practice%20Questions_2.png\" width=\"750,\" align=\"center\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRdFBLSlYBWL"
      },
      "source": [
        "Double-click __here__ for the solution.\n",
        "\n",
        "<!-- Your answer is below:\n",
        "conv5(imageA)+conv6(imageB)\n",
        "-->\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRD4bH86YBWL"
      },
      "source": [
        "\n",
        "<a href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01&context=cpdaas&apps=data_science_experience%2Cwatson_machine_learning\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBNbTOd9YBWL"
      },
      "source": [
        "### About the Authors:  \n",
        "[Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01) has a PhD in Electrical Engineering. His research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. \n",
        "\n",
        "Other contributors: [Michelle Carey](https://www.linkedin.com/in/michelleccarey/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01), [Mavis Zhou](https://www.linkedin.com/in/jiahui-mavis-zhou-a4537814a/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H83jA2hYBWL"
      },
      "source": [
        "\n",
        "## Change Log\n",
        "\n",
        "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
        "|---|---|---|---|\n",
        "| 2020-09-23  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
        "\n",
        "\n",
        "\n",
        "<hr>\n",
        "\n",
        "## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "conda-env-python-py"
    },
    "language_info": {
      "name": ""
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}